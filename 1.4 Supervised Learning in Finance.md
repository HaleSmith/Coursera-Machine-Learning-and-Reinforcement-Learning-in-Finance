# Course1: Guided Tour of Machine Learning in Finance
## Week4: Supervised Learning in Finance
### 1.4.1 Prediction of EPS
Figuire4.1: Types of stock analysis
![Types of stock analysis](https://github.com/SuperSaiki/pics/blob/master/MLinF35.png)

Figuire4.2: Forecast of EPS
![Forecast of EPS](https://github.com/SuperSaiki/pics/blob/master/MLinF36.png)

Figuire4.3: Variable Selecting for Value Investing
![Variable Selecting for Value Investing](https://github.com/SuperSaiki/pics/blob/master/MLinF37.png)

### 1.4.2 Machine Learning with Probalistic Models (Classification Models)

1.4.2.1 Machine Learning as MOdel Estimation

Suppose the f we estimate belong to a family F. If all f in F has a fixed number of parameters, then it is called parametric function fitting. If family has a data-dependent number of parameters, it is called non-parametric function fitting.

Figure4: Supervised Learning Algorithms
![Supervised Learning Algorithms](https://github.com/SuperSaiki/pics/blob/master/MLinF38.png)

Figure5: ML training for Probabilistic Models
![ML training for Probabilistic Models](https://github.com/SuperSaiki/pics/blob/master/MLinF39.png)

Figure6: MLE and MAP
![MLE and MAP](https://github.com/SuperSaiki/pics/blob/master/MLinF41.png)

Figure7: Why MLE
![Why MLE](https://github.com/SuperSaiki/pics/blob/master/MLinF43.png)

Figure8: MLE and Least Squared Loss
![MLE and Least Squared Loss](https://github.com/SuperSaiki/pics/blob/master/MLinF42.png)

Figure9: Kullback-Leibler (KL) Divergence
![Kullback-Leibler (KL) Divergence](https://github.com/SuperSaiki/pics/blob/master/MLinF44.png)

Figure10: MAP and Regularization 
![MAP and Regularization](https://github.com/SuperSaiki/pics/blob/master/MLinF45.png)

Figure11: Probabilistic Classification Models
![Probabilistic Classification Models](https://github.com/SuperSaiki/pics/blob/master/MLinF46.png)


Figure12: Logistic Regression
![Logistic Regression](https://github.com/SuperSaiki/pics/blob/master/MLinF47.png)

Figure13: MLE for Logistic Function
![MLE for Logistic Function](https://github.com/SuperSaiki/pics/blob/master/MLinF48.png)

Figure14: CAMELS Supervisory Rating Systems by FDIC
![CAMELS Supervisory Rating Systems by FDIC](https://github.com/SuperSaiki/pics/blob/master/MLinF49.png)

Figure15: Corporate Default: The Merton Model
![CAMELS Supervisory Rating Systems by FDIC](https://github.com/SuperSaiki/pics/blob/master/MLinF50.png)

Figure16: Merton Model as a Structural Default Model
![Merton Model as a Structural Default Model](https://github.com/SuperSaiki/pics/blob/master/MLinF51.png)

Figure17: Why Merton Approach is not good
![Why Merton Approach is not good](https://github.com/SuperSaiki/pics/blob/master/MLinF52.png)

Figure18: Prediction with Logistic Regression
![Prediction with Logistic Regression](https://github.com/SuperSaiki/pics/blob/master/MLinF53.png)

Figure19: 2D views of Data
![2D views of Data](https://github.com/SuperSaiki/pics/blob/master/MLinF54.png)

This shows that this problem is suitable for Logistic Regression.

Figure20: Results
![Results](https://github.com/SuperSaiki/pics/blob/master/MLinF55.png)

Figure21: Rules to solve regression or classification problem
![Rules to solve regression or classification problem](https://github.com/SuperSaiki/pics/blob/master/MLinF56.png)

[Tobit Regression-Wikipedia](https://en.wikipedia.org/wiki/Tobit_model)

In practice, in case out of range value, we always use log(0.00001 + ...) instead of log(...) in the MLE function. We also define <a href="https://www.codecogs.com/eqnedit.php?latex=\sigma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sigma" title="\sigma" /></a> as (0.0001 + <a href="https://www.codecogs.com/eqnedit.php?latex=(w[-1])^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(w[-1])^2" title="(w[-1])^2" /></a>).

[Week4 assignment:Tobit_regression_m1_ex3_v4-newV (1).ipynb](https://github.com/SuperSaiki/Coursera-Machine-Learning-and-Reinforcement-Learning-in-Finance/blob/master/Tobit_regression_m1_ex3_v4-newV%20(1).ipynb)

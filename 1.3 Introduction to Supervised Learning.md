# Course1: Guided Tour of Machine Learning in Finance
## Week3: Introduction to Neural Networks and TensorFlow

### 3.1 Dataflow Graph
Figuire 3.1:Dataflow Graph
![Dataflow Graph](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF21.png)
Tensor: Multidimentional Arrays. (mathematical objects)

Tensor + Dataflow = TensorFlow!

Figuire 3.2:Tensorflow: Lasy Evaluation
![Tensorflow:Lasy Evaluation](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF22.png)

Figuire 3.3:Reverse-Mode Autodiff
![Tensorflow:Reverse-Mode Autodiff](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF23.png)

### 3.2 A First Demo of TensorFlow
[See jupyter notebook09 in the book "hands on machine learning"](https://github.com/ageron/handson-ml/blob/master/09_up_and_running_with_tensorflow.ipynb)

Define a composite function:
![a composite function](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF24.png)

```python
def my_func(w,x):
  with tf.name_scope("f_0_level") as scope_0:
    f_0 = tf.exp(w[0,0]+w[0,1]*x)
  with tf.name_scope("f_1_level") as scope_1:
    f_1 = tf.exp(w[1,0]+w[1,1]*f_0)
  with tf.name_scope("f_2_level") as scope_2:
    f_2 = tf.exp(w[2,0]+w[2,1]*f1)
  
  return f2,f1,f0
```

python fucntion: stack(a,axis = 0 or 1 or 2 or... ) the number of choices of axis depends on the dimension of list a.

When axis = 0, take the first element of 'a' to be the first layer (element) of the new array. Take the second element of 'a' to be the second layer (element) of the new array. So on and so forth. (Can also be thought as via the x axis)

When axis = 1 (need 'a' to be at least two dimendional list), take the first element of every element of 'a' to be the first layer (element) of the new array.  Take the second element of every element of 'a' to be the second layer (element) of the new array.
So on and so forth. (Can also be thought as via the y axis)

When axis = 2 (need 'a' to be at least three dimendional list), take the first element of the first element of every element of 'a' to be the first layer (element) of the new array.  Take the second element of every element of 'a' to be the second layer (element) of the new array. (Can also be thought as via the z axis)

So on and so forth.

Intuitional examples:
![stack example1](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF26.png)
![stack example2](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF27.png)
Calculate the derivatives of f at w0:
![the derivatives of f at w0](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF25.png)

One mistake with the first line: af/aw0 = af/af2 * af2/aw20 = af2/aw20 = f2(w0) (Here 'a' means partial differentiation)

Other lines have the same problem.

Compute gradients using TensorFlow
![Compute gradients using TensorFlow part1](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF28.png)
![Compute gradients using TensorFlow part2](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF29.png)

[tensorflow_graph_in_jupyter](https://github.com/ageron/handson-ml/blob/master/tensorflow_graph_in_jupyter.py)


### 3.3 Linear Regression in TensorFlow

Artificial Neuron
![Compute gradients using TensorFlow part2](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF30.png)

Compound function transform with a Neuron
![Compound function transform with a Neuron](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF31.png)

Artificial Neural networks
![Artificial Neural networks](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF32.png)

More than 2 hidden layers = Deep Neural Network (Deep Learning). Usually trained by Gradient Descent.

### 3.4 Gradient Descent Optimization
Back Propagation = Gradient Descent with a reverse mode autodiff.

Gradient Descent with Back Propagation
![Gradient Descent with Back Propagation](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF33.png)

### 3.5 Stochastic Gradient Discent

Stochastic Gradient Descent.

![Stochastic Gradient Descent](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF34.png)

The last function is also called Langevin Equation in physics.

Adcantages:

1) A faster evaluation over a single step of gradient descent. (Just a portion of the whole data set). Especially when the data set is large.

2) Attempting to directly minimize the generalization loss. (Gradient descent only tries to minimize the training error of the fixed training set.)

Why is it so?

This is because each time we randomly sample a next mini-batch, we actually sample from a two data generating distribution. Therefore, when we minimize the training error on these mini-batch, we actually estimate the generalization error. In other words, we can say that stochastic gradient descent minimizes the generalization error plus random noise. The last term epsilon k, stands for numerical noise due to the choice of a particular mini-batch on the k iteration.

The presence of random noises have some profound consequences for learning.

3) First, it often helps to escape local minimum in training. (Has a chance to escape it due to a random fluctuation in the mini-batch.)

At the biginning, the first deterministic thing is usually very big compared with the second one. As this algorithm processing, the first one become very small compared with the second random term.

On-line SGD usually converge slowly.

[Programming assingment2: linear_regress_m1_ex2_v4-newV ](https://github.com/SuperSaiki/Coursera-Machine-Learning-and-Reinforcement-Learning-in-Finance/blob/master/linear_regress_m1_ex2_v4-newV%20(1).ipynb)

Notice: The initial shape of tf.Variable is not important, it could change with the data feed to it.

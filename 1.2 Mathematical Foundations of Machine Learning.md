# Course1: Guided Tour of Machine Learning in Finance
## Week2: Mathematical Foundations of Machine Learning

### 1.2.1 Generalization and a Bias-Variance Tradeoff

Figure2.1: Generalization Error in Regression
![Generalization Error in Regression](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF10.png)

Figure2.2: The Bias-Variance Decomposition
![The Bias-Variance Decomposition](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF11.png)
Note that: Variance is a function of f hat, not f. Variance is due to the change of the estimated parameters. When you use different choice of data to train the model, you will get different parameters. Then your prediction will vary when you use different parameters to do the prediction. That is variance.

In practice there is a negetive correlation between bias and variances. Complex model usually has a low bias because it is more flexible with more parameters. But it will have a high variance due to a large number of parameters. For simple model, it is the opposite one, with low variance but high bias.

**It's important to control model complexity to have an optimal trade-off!**

### 1.2.2 No Free Lunch Theorem
Figure2.3: No Free Lunch Theorem
![No Free Lunch Theorem](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF12.png)

### 1.2.3 Overfitting and Model Capacity
Figure2.4: Overfitting and Underfitting
![Overfitting and Underfitting](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF13.png)

Figure2.5: Model Capacity and Overfitting
![Model Capacity and Overfitting](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF14.png)

### 1.2.4 Linear Regression
Figure2.6: Linear Regression: Task
![Linear Regression: Task](https://raw.githubusercontent.com/SuperSaiki/pics/master/MLinF15.png)
